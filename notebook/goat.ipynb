{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:23:00.336967Z",
     "iopub.status.busy": "2026-01-30T18:23:00.336723Z",
     "iopub.status.idle": "2026-01-30T18:23:13.819625Z",
     "shell.execute_reply": "2026-01-30T18:23:13.818947Z",
     "shell.execute_reply.started": "2026-01-30T18:23:00.336933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.2/485.2 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\n",
      "google-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "fastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes langchain langchain-community \\\n",
    "    langchain-huggingface faiss-cpu fastapi uvicorn pyngrok \\\n",
    "    youtube-transcript-api pymupdf sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:23:13.820961Z",
     "iopub.status.busy": "2026-01-30T18:23:13.820661Z",
     "iopub.status.idle": "2026-01-30T18:23:45.030162Z",
     "shell.execute_reply": "2026-01-30T18:23:45.029568Z",
     "shell.execute_reply.started": "2026-01-30T18:23:13.820924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-30 18:23:28.289229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769797408.499240      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769797408.559390      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769797409.072576      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769797409.072617      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769797409.072620      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769797409.072623      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import threading\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import uvicorn\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "import fitz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:23:45.031752Z",
     "iopub.status.busy": "2026-01-30T18:23:45.031094Z",
     "iopub.status.idle": "2026-01-30T18:23:45.159907Z",
     "shell.execute_reply": "2026-01-30T18:23:45.159395Z",
     "shell.execute_reply.started": "2026-01-30T18:23:45.031725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:23:45.160975Z",
     "iopub.status.busy": "2026-01-30T18:23:45.160766Z",
     "iopub.status.idle": "2026-01-30T18:23:48.524892Z",
     "shell.execute_reply": "2026-01-30T18:23:48.524166Z",
     "shell.execute_reply.started": "2026-01-30T18:23:45.160953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=secret_value_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:23:48.526484Z",
     "iopub.status.busy": "2026-01-30T18:23:48.526201Z",
     "iopub.status.idle": "2026-01-30T18:26:36.915424Z",
     "shell.execute_reply": "2026-01-30T18:26:36.914872Z",
     "shell.execute_reply.started": "2026-01-30T18:23:48.526454Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38921404eece4f8b86e56da47ddc120e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51e4d616c8641c2a4b7b979b83e0a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b4b38c59e04932a4310d25060b986b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c42dd2b7ba46f3ada58e4a46fb543c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d61ccf16d0544a099f5c560feb3ea06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84542a3626a4eee817079cede8deb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a03959ae9824a3cbe9372734ea67fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b04aa66865a475f9e15a330663eea4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca5294f59d44775b15cd92883cbb18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b074a53e5894181b686b08b9ea3528e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c27bcdbb18407d902e6eaa5880d6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b304d6a4efb3472eb4dcd84bf5280d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_55/2604756406.py:48: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_gen_pipe)\n"
     ]
    }
   ],
   "source": [
    "NGROK_TOKEN = \"31CaKcK72zQFZ5v6dgDeAWmtRTg_T8zD7FHZNe1WYBKrsNwN\" #paste your NGROK Token from the website\n",
    "API_KEY = \"secret1234\"\n",
    "PORT = 8000\n",
    "FAISS_PATH = \"/kaggle/working/faiss_index\"\n",
    "PDF_FAISS_PATH = \"/kaggle/working/pdf_faiss_index\"\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "text_gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_gen_pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:26:36.917903Z",
     "iopub.status.busy": "2026-01-30T18:26:36.917267Z",
     "iopub.status.idle": "2026-01-30T18:26:36.923780Z",
     "shell.execute_reply": "2026-01-30T18:26:36.923013Z",
     "shell.execute_reply.started": "2026-01-30T18:26:36.917878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConversationHistory:\n",
    "    def __init__(self, max_history=10):\n",
    "        self.max_history = max_history\n",
    "        self.history: List[Dict] = []\n",
    "    \n",
    "    def add_exchange(self, user_msg: str, assistant_msg: str):\n",
    "        \"\"\"Add a complete exchange\"\"\"\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        \n",
    "        if len(self.history) > self.max_history * 2:\n",
    "            self.history = self.history[-self.max_history * 2:]\n",
    "    \n",
    "    def get_context_string(self) -> str:\n",
    "        \"\"\"Get formatted history for context\"\"\"\n",
    "        if not self.history:\n",
    "            return \"\"\n",
    "        \n",
    "        context = \"\\n\\nPrevious conversation:\\n\"\n",
    "        # Use last 3 exchanges (6 messages)\n",
    "        for msg in self.history[-6:]:\n",
    "            role = \"Human\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            context += f\"{role}: {msg['content']}\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def clear(self):\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:26:36.927877Z",
     "iopub.status.busy": "2026-01-30T18:26:36.927291Z",
     "iopub.status.idle": "2026-01-30T18:26:36.940231Z",
     "shell.execute_reply": "2026-01-30T18:26:36.939480Z",
     "shell.execute_reply.started": "2026-01-30T18:26:36.927854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, system_prompt: str = \"You are GOAT, a helpful and knowledgeable AI assistant.\", \n",
    "                     max_tokens: int = 1024) -> str:\n",
    "    \"\"\"Generate response without storing history internally\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    output = text_gen_pipe(\n",
    "        formatted_prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        return_full_text=False\n",
    "    )\n",
    "    \n",
    "    return output[0][\"generated_text\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:26:36.952318Z",
     "iopub.status.busy": "2026-01-30T18:26:36.951990Z",
     "iopub.status.idle": "2026-01-30T18:26:36.963814Z",
     "shell.execute_reply": "2026-01-30T18:26:36.963304Z",
     "shell.execute_reply.started": "2026-01-30T18:26:36.952295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "study_plan_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject\", \"level\", \"time_available\", \"goal\", \"timeline\"],\n",
    "    template=\"\"\"You are GOAT, an expert educational consultant. Create a comprehensive study plan.\n",
    "\n",
    "Subject: {subject}\n",
    "Level: {level}\n",
    "Time Available: {time_available} hours/week\n",
    "Goal: {goal}\n",
    "Timeline: {timeline}  # Timeline will be a natural description like '1 month', '3 months', '6 months', or '1 year'\n",
    "\n",
    "GUIDELINES:\n",
    "- Based on {level} and {goal}\n",
    "- Do NOT convert the timeline into weeks; use the timeline as a guide for pacing.\n",
    "- Divide learning into logical phases: Foundation â†’ Intermediate â†’ Mastery.\n",
    "- Suggest milestones and goals for each phase relative to the timeline.\n",
    "- Suggest weekly/daily routines without assigning fixed week numbers.\n",
    "- Make the plan concise, actionable, and structured.\n",
    "\n",
    "WEEKLY BREAKDOWN\n",
    "- Suggest topics and focus areas per week without using hard-coded week numbers\n",
    "\n",
    "DAILY SCHEDULE\n",
    "- How to distribute {time_available} hours/week\n",
    "- Monday-Friday routine\n",
    "- Weekend activities\n",
    "\n",
    "MILESTONES\n",
    "- Define key achievements for each phase (Foundation, Intermediate, Mastery)\n",
    "- Use the timeline to scale milestones appropriately\n",
    "\n",
    "LEARNING RESOURCES\n",
    "- Top 3 online courses\n",
    "- Top 3 books\n",
    "- Top 3 practice platforms\n",
    "\n",
    "Keep sections clearly separated with single line breaks.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "study_plan_chain = study_plan_prompt | llm | StrOutputParser()\n",
    "\n",
    "roadmap_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject\", \"level\", \"goal\"],\n",
    "    template=\"\"\"You are GOAT, a learning path architect.\n",
    "\n",
    "Create a structured and practical learning roadmap.\n",
    "\n",
    "Subject: {subject}\n",
    "Starting Level: {level}\n",
    "Goal: {goal}\n",
    "\n",
    "GUIDELINES:\n",
    "- Based on {level} and {goal}\n",
    "- Divide the learning journey into clear phases:\n",
    "  Foundation â†’ Intermediate â†’ Mastery\n",
    "- Focus on progression of skills, not duration\n",
    "- Do NOT mention specific weeks or months\n",
    "- Keep the roadmap flexible and adaptable\n",
    "- Emphasize practical skills and real-world application\n",
    "\n",
    "PHASE 1 â€“ FOUNDATION\n",
    "- Core concepts to understand\n",
    "- Fundamental skills to build\n",
    "- Beginner-level practice ideas\n",
    "\n",
    "PHASE 2 â€“ INTERMEDIATE\n",
    "- More advanced topics\n",
    "- Hands-on projects\n",
    "- Skills needed for real use cases\n",
    "\n",
    "PHASE 3 â€“ MASTERY\n",
    "- Expert-level concepts\n",
    "- Real-world and portfolio projects\n",
    "- How this phase moves the learner closer to the goal\n",
    "\n",
    "Use concise bullet points.\n",
    "Keep the roadmap clear, actionable, and complete.\n",
    "Use single line breaks between sections.\"\"\"\n",
    ")\n",
    "\n",
    "roadmap_chain = roadmap_prompt | llm | StrOutputParser()\n",
    "\n",
    "resources_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject\", \"goal\"],\n",
    "    template=\"\"\"You are GOAT, a learning resource curator.\n",
    "\n",
    "Recommend the best resources for a learner with the following goal:\n",
    "\n",
    "Subject: {subject}\n",
    "Goal: {goal}\n",
    "\n",
    "GUIDELINES:\n",
    "- Tailor resources to the learner's timeline and goal\n",
    "- Prioritize high-quality and actionable materials\n",
    "- Include specific recommendations for each type\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "\n",
    "ONLINE COURSES (Top 3)\n",
    "1. Course name - Platform - Brief description\n",
    "2. \n",
    "3. \n",
    "\n",
    "BOOKS (Top 3)\n",
    "1. Title - Author - Why valuable\n",
    "2.\n",
    "3.\n",
    "\n",
    "PRACTICE PLATFORMS (Top 3)\n",
    "1. Platform - What to practice\n",
    "2.\n",
    "3.\n",
    "\n",
    "COMMUNITIES (Top 2)\n",
    "1. Community - How it helps\n",
    "2.\n",
    "\n",
    "Keep the recommendations structured and concise.\n",
    "Use single line breaks between sections.\"\"\"\n",
    ")\n",
    "\n",
    "resources_chain = resources_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:26:36.964894Z",
     "iopub.status.busy": "2026-01-30T18:26:36.964623Z",
     "iopub.status.idle": "2026-01-30T18:26:36.980205Z",
     "shell.execute_reply": "2026-01-30T18:26:36.979506Z",
     "shell.execute_reply.started": "2026-01-30T18:26:36.964848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "youtube_summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"transcript\"],\n",
    "    template=\"\"\"You are GOAT. Summarize this video transcript comprehensively.\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\n",
    "Create a structured summary with:\n",
    "\n",
    "MAIN TOPIC\n",
    "- Brief description\n",
    "\n",
    "KEY POINTS\n",
    "- Point 1\n",
    "- Point 2\n",
    "- Point 3\n",
    "- Point 4\n",
    "- Point 5\n",
    "\n",
    "IMPORTANT DETAILS\n",
    "- Detail 1\n",
    "- Detail 2\n",
    "- Detail 3\n",
    "\n",
    "CONCLUSIONS\n",
    "- Final takeaway\n",
    "\n",
    "Keep it clear and well-organized. Use single line breaks between sections.\"\"\"\n",
    ")\n",
    "\n",
    "youtube_summary_chain = youtube_summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "youtube_chunk_prompt = PromptTemplate(\n",
    "    input_variables=[\"chunk\"],\n",
    "    template=\"\"\"Summarize this video segment in 3-4 concise bullet points:\n",
    "\n",
    "{chunk}\"\"\"\n",
    ")\n",
    "\n",
    "youtube_chunk_chain = youtube_chunk_prompt | llm | StrOutputParser()\n",
    "\n",
    "youtube_reduce_prompt = PromptTemplate(\n",
    "    input_variables=[\"summaries\"],\n",
    "    template=\"\"\"Combine these segment summaries into one coherent report:\n",
    "\n",
    "{summaries}\n",
    "\n",
    "Format:\n",
    "\n",
    "MAIN TOPIC\n",
    "- Brief description\n",
    "\n",
    "KEY POINTS\n",
    "- Point 1\n",
    "- Point 2\n",
    "- Point 3\n",
    "- Point 4\n",
    "- Point 5\n",
    "\n",
    "IMPORTANT DETAILS\n",
    "- Detail 1\n",
    "- Detail 2\n",
    "- Detail 3\n",
    "\n",
    "CONCLUSIONS\n",
    "- Final takeaway\n",
    "\n",
    "Keep it concise and well-structured. Use single line breaks between sections.\"\"\"\n",
    ")\n",
    "\n",
    "youtube_reduce_chain = youtube_reduce_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:26:36.981466Z",
     "iopub.status.busy": "2026-01-30T18:26:36.981141Z",
     "iopub.status.idle": "2026-01-30T18:26:37.001003Z",
     "shell.execute_reply": "2026-01-30T18:26:37.000383Z",
     "shell.execute_reply.started": "2026-01-30T18:26:36.981433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_video_id(url: str) -> str:\n",
    "    \"\"\"Extract YouTube video ID from URL\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.hostname in ['youtu.be']:\n",
    "        return parsed.path[1:]\n",
    "    if parsed.hostname in ['www.youtube.com', 'youtube.com']:\n",
    "        if parsed.path == '/watch':\n",
    "            return parse_qs(parsed.query)['v'][0]\n",
    "        if parsed.path.startswith(('/embed/', '/v/')):\n",
    "            return parsed.path.split('/')[2]\n",
    "    raise ValueError(f\"Could not extract video ID from URL: {url}\")\n",
    "\n",
    "def get_transcript(video_id: str) -> str:\n",
    "    \"\"\"Fetch YouTube transcript\"\"\"\n",
    "    try:\n",
    "        api = YouTubeTranscriptApi()\n",
    "        fetched = api.fetch(video_id)  # returns a FetchedTranscript\n",
    "        text = \"\\n\".join(snippet.text for snippet in fetched)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_bytes: bytes) -> str:\n",
    "    \"\"\"Extract text from PDF bytes\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        \n",
    "        if not text.strip():\n",
    "            return \"Error: PDF appears to be empty or contains only images\"\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting PDF text: {str(e)}\"\n",
    "\n",
    "\n",
    "def setup_rag(text_content: str, index_path: str):\n",
    "    \"\"\"Build and persist FAISS vector store\"\"\"\n",
    "    docs = [Document(page_content=text_content, metadata={\"source\": \"document\"})]\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    \n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    vectordb = FAISS.from_documents(chunks, embedding)\n",
    "    vectordb.save_local(index_path)\n",
    "    \n",
    "    return vectordb\n",
    "\n",
    "def load_rag(index_path: str):\n",
    "    \"\"\"Load existing FAISS index\"\"\"\n",
    "    if os.path.exists(index_path):\n",
    "        return FAISS.load_local(\n",
    "            index_path,\n",
    "            HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            ),\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "    return None\n",
    "\n",
    "def chunk_transcript(transcript: str, words_per_chunk: int = 5000):\n",
    "    \"\"\"Split transcript into larger chunks for fewer API calls\"\"\"\n",
    "    words = transcript.split()\n",
    "    for i in range(0, len(words), words_per_chunk):\n",
    "        yield \" \".join(words[i:i + words_per_chunk])\n",
    "\n",
    "def process_summarization(full_transcript: str) -> str:\n",
    "    \"\"\"Process video summarization using LangChain chains\"\"\"\n",
    "    clean_text = re.sub(r'\\[.*?\\]', '', full_transcript)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    \n",
    "    chunks = list(chunk_transcript(clean_text, words_per_chunk=5000)) \n",
    "    \n",
    "    print(f\"ðŸ”„ Summarizing {len(chunks)} segments...\")\n",
    "    \n",
    "    if len(chunks) <= 2:\n",
    "        print(\"ðŸ“ Short video - using direct summary chain...\")\n",
    "        summary = youtube_summary_chain.invoke({\"transcript\": clean_text[:15000]})\n",
    "        return summary\n",
    "    \n",
    "    chunk_summaries = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"ðŸ“ Processing segment {i+1}/{len(chunks)} with chain...\")\n",
    "        summary = youtube_chunk_chain.invoke({\"chunk\": chunk})\n",
    "        chunk_summaries.append(summary)\n",
    "    \n",
    "    print(\"ðŸ Synthesizing final report with reduce chain...\")\n",
    "    combined_text = \"\\n\\n\".join(chunk_summaries)\n",
    "    \n",
    "    final_summary = youtube_reduce_chain.invoke({\"summaries\": combined_text})\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:26:37.002180Z",
     "iopub.status.busy": "2026-01-30T18:26:37.001927Z",
     "iopub.status.idle": "2026-01-30T18:26:37.054505Z",
     "shell.execute_reply": "2026-01-30T18:26:37.053686Z",
     "shell.execute_reply.started": "2026-01-30T18:26:37.002159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AppState:\n",
    "    def __init__(self):\n",
    "        # YouTube\n",
    "        self.youtube_vectordb = None\n",
    "        self.youtube_transcript = None\n",
    "        self.youtube_history = ConversationHistory(max_history=10)\n",
    "        \n",
    "        # PDF\n",
    "        self.pdf_vectordb = None\n",
    "        self.pdf_text = None\n",
    "        self.pdf_history = ConversationHistory(max_history=10)\n",
    "        \n",
    "        # General Chat\n",
    "        self.general_history = ConversationHistory(max_history=15)\n",
    "\n",
    "state = AppState()\n",
    "\n",
    "\n",
    "app = FastAPI(title=\"AI Assistant Backend\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def health():\n",
    "    return {\n",
    "        \"status\": \"online\",\n",
    "        \"model\": \"Llama-3.1-8B-Instruct\",\n",
    "        \"features\": [\"study_plan\", \"youtube\", \"pdf_qa\", \"chat\"],\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "@app.post(\"/study-plan/generate\")\n",
    "async def create_study_plan(\n",
    "    request: Request,\n",
    "    subject: str = Form(...),\n",
    "    level: str = Form(...),\n",
    "    time_available: str = Form(...),\n",
    "    goal: str = Form(...),\n",
    "    timeline: str = Form(...)\n",
    "):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        temp_pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            max_new_tokens=4096, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        temp_llm = HuggingFacePipeline(pipeline=temp_pipe)\n",
    "        temp_chain = study_plan_prompt | temp_llm | StrOutputParser()\n",
    "        \n",
    "        study_plan = temp_chain.invoke({\n",
    "            \"subject\": subject,\n",
    "            \"level\": level,\n",
    "            \"time_available\": time_available,\n",
    "            \"goal\": goal,\n",
    "            \"timeline\": timeline\n",
    "        })\n",
    "        \n",
    "        study_plan = re.sub(r'\\n{3,}', '\\n\\n', study_plan)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"study_plan\": study_plan,\n",
    "            \"timestamp\": end_time.isoformat(),\n",
    "            \"generation_time\": f\"{duration:.2f} seconds\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/study-plan/roadmap\")\n",
    "async def create_roadmap(\n",
    "    request: Request,\n",
    "    subject: str = Form(...),\n",
    "    level: str = Form(...),\n",
    "    goal: str = Form(...)\n",
    "):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        temp_pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            max_new_tokens=4096,  \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        temp_llm = HuggingFacePipeline(pipeline=temp_pipe)\n",
    "        temp_chain = roadmap_prompt | temp_llm | StrOutputParser()\n",
    "        \n",
    "        roadmap = temp_chain.invoke({\n",
    "            \"subject\": subject,\n",
    "            \"level\": level,\n",
    "            \"goal\": goal\n",
    "        })\n",
    "        \n",
    "        roadmap = re.sub(r'\\n{3,}', '\\n\\n', roadmap)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"roadmap\": roadmap,\n",
    "            \"timestamp\": end_time.isoformat(),\n",
    "            \"generation_time\": f\"{duration:.2f} seconds\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/study-plan/resources\")\n",
    "async def get_resources(\n",
    "    request: Request,\n",
    "    subject: str = Form(...),\n",
    "    goal: str = Form(...)\n",
    "):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        resources = resources_chain.invoke({\n",
    "            \"subject\": subject,\n",
    "            \"goal\": goal\n",
    "        })\n",
    "        \n",
    "        resources = re.sub(r'\\n{3,}', '\\n\\n', resources)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"resources\": resources,\n",
    "            \"timestamp\": end_time.isoformat(),\n",
    "            \"generation_time\": f\"{duration:.2f} seconds\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/youtube/process\")\n",
    "async def process_youtube(request: Request, url: str = Form(...), transcript: str = Form(...)):\n",
    "    \"\"\"UPDATED: Now accepts transcript from frontend instead of fetching\"\"\"\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    try:\n",
    "        if not transcript or transcript.startswith(\"Error\"):\n",
    "            return {\"success\": False, \"error\": \"Invalid transcript provided\"}\n",
    "        \n",
    "        state.youtube_transcript = transcript\n",
    "        state.youtube_vectordb = setup_rag(transcript, FAISS_PATH)\n",
    "        state.youtube_history.clear()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"Video processed successfully\",\n",
    "            \"transcript_length\": len(transcript),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/youtube/summarize\")\n",
    "async def summarize_youtube(request: Request):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    if not state.youtube_transcript:\n",
    "        return {\"success\": False, \"error\": \"No video processed.\"}\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        summary = process_summarization(state.youtube_transcript)\n",
    "        \n",
    "        summary = re.sub(r'\\n{3,}', '\\n\\n', summary)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"summary\": summary,\n",
    "            \"timestamp\": end_time.isoformat(),\n",
    "            \"generation_time\": f\"{duration:.2f} seconds\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/youtube/chat\")\n",
    "async def chat_youtube(request: Request, question: str = Form(...)):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    if not state.youtube_vectordb:\n",
    "        return {\"success\": False, \"error\": \"No video processed.\"}\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        docs = state.youtube_vectordb.similarity_search(question, k=3)\n",
    "        context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "        \n",
    "        history_context = state.youtube_history.get_context_string()\n",
    "        \n",
    "        full_prompt = f\"\"\"Context from video:\n",
    "{context}\n",
    "{history_context}\n",
    "\n",
    "Current question: {question}\n",
    "\n",
    "Answer based on the video content. Be specific and reference relevant information.\"\"\"\n",
    "        \n",
    "        answer = generate_response(full_prompt, max_tokens=800)\n",
    "        \n",
    "        state.youtube_history.add_exchange(question, answer)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"answer\": answer,\n",
    "            \"timestamp\": end_time.isoformat(),\n",
    "            \"generation_time\": f\"{duration:.2f} seconds\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/youtube/clear-history\")\n",
    "async def clear_youtube_history(request: Request):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    state.youtube_history.clear()\n",
    "    return {\"success\": True, \"message\": \"History cleared\"}\n",
    "\n",
    "@app.post(\"/pdf/upload\")\n",
    "async def upload_pdf(request: Request, file: UploadFile = File(...)):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    try:\n",
    "        pdf_bytes = await file.read()\n",
    "        text = extract_text_from_pdf(pdf_bytes)\n",
    "        \n",
    "        if text.startswith(\"Error\"):\n",
    "            return {\"success\": False, \"error\": text}\n",
    "        \n",
    "        state.pdf_text = text\n",
    "        state.pdf_vectordb = setup_rag(text, PDF_FAISS_PATH)\n",
    "        state.pdf_history.clear()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"message\": \"PDF processed successfully\",\n",
    "            \"text_length\": len(text),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/pdf/ask\")\n",
    "async def ask_pdf(request: Request, question: str = Form(...)):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    if not state.pdf_vectordb:\n",
    "        return {\"success\": False, \"error\": \"No PDF uploaded.\"}\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        docs = state.pdf_vectordb.similarity_search(question, k=4)\n",
    "        context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "        \n",
    "        history_context = state.pdf_history.get_context_string()\n",
    "        \n",
    "        full_prompt = f\"\"\"Context from PDF:\n",
    "{context}\n",
    "{history_context}\n",
    "\n",
    "Current question: {question}\n",
    "\n",
    "Answer based on the PDF content. Be accurate and reference specific information.\"\"\"\n",
    "        \n",
    "        answer = generate_response(full_prompt, max_tokens=800)\n",
    "        \n",
    "        state.pdf_history.add_exchange(question, answer)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"answer\": answer,\n",
    "            \"timestamp\": end_time.isoformat(),\n",
    "            \"generation_time\": f\"{duration:.2f} seconds\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/pdf/clear-history\")\n",
    "async def clear_pdf_history(request: Request):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    state.pdf_history.clear()\n",
    "    return {\"success\": True, \"message\": \"History cleared\"}\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "async def general_chat(request: Request, message: str = Form(...)):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        history_context = state.general_history.get_context_string()\n",
    "        \n",
    "        full_prompt = f\"\"\"{history_context}\n",
    "\n",
    "Current message: {message}\n",
    "\n",
    "Respond helpfully and naturally.\"\"\"\n",
    "        \n",
    "        response = generate_response(\n",
    "            full_prompt,\n",
    "            system_prompt=\"You are GOAT, a helpful and friendly AI assistant. Be conversational and informative.\",\n",
    "            max_tokens=800\n",
    "        )\n",
    "        \n",
    "        state.general_history.add_exchange(message, response)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": response,\n",
    "            \"timestamp\": end_time.isoformat(),\n",
    "            \"generation_time\": f\"{duration:.2f} seconds\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "@app.post(\"/chat/clear-history\")\n",
    "async def clear_chat_history(request: Request):\n",
    "    if request.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401)\n",
    "    state.general_history.clear()\n",
    "    return {\"success\": True, \"message\": \"History cleared\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T18:26:37.055710Z",
     "iopub.status.busy": "2026-01-30T18:26:37.055478Z",
     "iopub.status.idle": "2026-01-30T18:26:38.448562Z",
     "shell.execute_reply": "2026-01-30T18:26:38.447465Z",
     "shell.execute_reply.started": "2026-01-30T18:26:37.055688Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting server with NGROK...\n",
      "                                                                                                    \n",
      "âœ… BACKEND URL: https://ungoaded-subdorsally-bethanie.ngrok-free.dev\n",
      "ðŸ”‘ API KEY: secret1234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [55]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.202.70.116:0 - \"POST /study-plan/roadmap HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.202.70.116:0 - \"POST /study-plan/generate HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.202.70.116:0 - \"POST /study-plan/roadmap HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"POST /study-plan/resources HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38c8e131e8f467ead2680963cfa2dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cf66dff9fe4e21ba5a588ec3a732e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef46d326574d487db7d27dbb3d25649d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef4e51ac3fd4efc989ddb55ade03970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c625fb7682449239381d950ec93debd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9045b4da27e46d5b7fe485ca02f537c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b89ae51b0164ca4aa1165d45055690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127f8a4d1b6d4f2c8004d71d175911b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bb04877a7e4ab09589e8a05c289228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbdff75d96b40e099d611b5beb8ab7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911754c8a2b3494983be11dbb8d3692c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.202.70.116:0 - \"POST /youtube/process HTTP/1.1\" 200 OK\n",
      "ðŸ”„ Summarizing 1 segments...\n",
      "ðŸ“ Short video - using direct summary chain...\n",
      "INFO:     156.202.70.116:0 - \"POST /youtube/summarize HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"POST /youtube/chat HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"POST /youtube/chat HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"POST /pdf/upload HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"POST /pdf/ask HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"POST /pdf/ask HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
      "INFO:     156.202.70.116:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸš€ Starting server with NGROK...\")\n",
    "\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "\n",
    "try:\n",
    "    ngrok.kill()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "public_url = ngrok.connect(PORT).public_url\n",
    "print(f\"\\nâœ… BACKEND URL: {public_url}\")\n",
    "print(f\"ðŸ”‘ API KEY: {API_KEY}\\n\")\n",
    "\n",
    "def run():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
    "\n",
    "threading.Thread(target=run, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
